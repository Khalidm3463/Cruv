{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataaset into a DataFrame\n",
    "news_df = pd.read_csv(\"news.csv\")\n",
    "\n",
    "# Fill the missing values with an empty string\n",
    "news_df[\"content\"] = news_df[\"content\"].fillna(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Note - This step may take several minutes to run depending on the GPU you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Remove unwanted characters and stopwords, and convert the text to lowercase.\n",
    "    \"\"\"\n",
    "    # spacy.prefer_gpu(gpu_id=2)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = \" \".join(token.lemma_ for token in doc if not token.is_stop and not token.is_punct)\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "# Apply the preprocess_text function to the \"content\" column\n",
    "news_df[\"content\"] = news_df[\"content\"].apply(preprocess_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_sentences(text, top_n=3):\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split(\".\")\n",
    "    \n",
    "    # Remove empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    if not sentences:\n",
    "        return \"\", \"\", {\"error\": \"No sentences found in text\"}\n",
    "    \n",
    "    # Vectorize the sentences\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    # Compute the sentence similarity matrix\n",
    "    sim_matrix = cosine_similarity(sentence_vectors)\n",
    "    \n",
    "    # Sort the sentence similarity matrix in descending order\n",
    "    sim_matrix_sorted = np.argsort(-sim_matrix)\n",
    "    \n",
    "    # Select the top n most important sentences\n",
    "    top_sentences_idx = sim_matrix_sorted[:top_n]\n",
    "    \n",
    "    # Get the actual sentences corresponding to the top n indices\n",
    "    top_sentences = []\n",
    "    for idx in top_sentences_idx:\n",
    "        if idx.size > 0 and idx[0] < len(sentences):\n",
    "            top_sentences.append(sentences[idx[0]])\n",
    "    \n",
    "    # Remove the top n most important sentences from the document\n",
    "    removed_lines = \"\\n\".join(sentence for sentence in sentences if sentence.strip() not in top_sentences)\n",
    "    \n",
    "    # Generate a summary of the top n sentences\n",
    "    summary = {\"top_sentences\": top_sentences}\n",
    "    \n",
    "    return removed_lines, top_sentences, summary\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_set, test_set = train_test_split(news_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Rank the sentences in the test set and store the results in a dataframe\n",
    "results = pd.DataFrame(columns=[\"Original Content\", \"New Content\", \"Removed Lines\", \"Further Metrics\"])\n",
    "for index, row in test_set.iterrows():\n",
    "    original_content = row[\"content\"]\n",
    "    new_content, removed_lines, summary = rank_sentences(original_content)\n",
    "    if not new_content:\n",
    "        continue\n",
    "    new_row = pd.DataFrame({\"Original Content\": [original_content],\n",
    "                            \"New Content\": [new_content],\n",
    "                            \"Removed Lines\": [removed_lines],\n",
    "                            \"Further Metrics\": [summary]})\n",
    "    results = pd.concat([results, new_row], ignore_index=True)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results.to_csv(\"summary.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
