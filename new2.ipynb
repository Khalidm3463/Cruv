{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "news_df = pd.read_csv(\"news.csv\")\n",
    "\n",
    "# Fill any missing values with an empty string\n",
    "news_df[\"content\"] = news_df[\"content\"].fillna(\"\")\n",
    "\n",
    "# Define a function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Remove unwanted characters and stopwords, and convert the text to lowercase.\n",
    "    \"\"\"\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = \" \".join(token.lemma_ for token in doc if not token.is_stop and not token.is_punct)\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "# Apply the preprocess_text function to the \"content\" column\n",
    "news_df[\"content\"] = news_df[\"content\"].apply(preprocess_text)\n",
    "\n",
    "# Define a function to rank the sentences in a document based on their importance\n",
    "def rank_sentences(text, top_n=5):\n",
    "    \"\"\"\n",
    "    Extract the top n most important sentences from a document based on their TF-IDF scores.\n",
    "    \"\"\"\n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Compute the TF-IDF scores for each sentence in the document\n",
    "    sentence_scores = []\n",
    "    for sentence in text.split(\".\"):\n",
    "        if sentence:\n",
    "            sentence_scores.append(vectorizer.fit_transform([sentence]).todense())\n",
    "\n",
    "    # Convert the sentence scores to a numpy array\n",
    "    sentence_scores = np.array(sentence_scores)\n",
    "\n",
    "    # Calculate the average TF-IDF score for each sentence\n",
    "    avg_scores = np.mean(sentence_scores, axis=1)\n",
    "\n",
    "    # Sort the sentences by their average TF-IDF scores in descending order\n",
    "    top_sentences_idx = np.argsort(avg_scores)[::-1][:top_n]\n",
    "\n",
    "    # Extract the top n most important sentences from the document\n",
    "    top_sentences = []\n",
    "    for idx in top_sentences_idx:\n",
    "        top_sentences.append(text.split(\".\")[int(idx)])\n",
    "\n",
    "\n",
    "    # Remove the top n most important sentences from the document\n",
    "    removed_lines = \"\\n\".join(sentence for sentence in text.split(\".\") if sentence.strip() not in top_sentences)\n",
    "\n",
    "    # Combine the remaining sentences into a new document\n",
    "    new_content = \".\".join(sentence for sentence in top_sentences)\n",
    "\n",
    "    # Calculate the cosine similarity between the sentence embeddings\n",
    "    sentence_vectors = np.array([vectorizer.fit_transform([sentence]).todense() for sentence in top_sentences])\n",
    "    sim_matrix = cosine_similarity(sentence_vectors)\n",
    "\n",
    "    # Calculate the sentence scores by summing the cosine similarities for each sentence\n",
    "    scores = np.sum(sim_matrix, axis=1)\n",
    "\n",
    "    # Normalize the scores\n",
    "    scores = scores / np.sum(scores)\n",
    "\n",
    "    # Create a dictionary of the top n most important sentences and their scores\n",
    "    summary = {}\n",
    "    for i in range(top_n):\n",
    "        summary[top_sentences[i]] = scores[i]\n",
    "\n",
    "    return new_content, removed_lines, summary\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_set, test_set = train_test_split(news_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# Rank the sentences in the test set and store the results in a dataframe\n",
    "results = pd.DataFrame(columns=[\"Original Content\", \"New Content\", \"Removed Lines\", \"Further Metrics\"])\n",
    "for index, row in test_set.iterrows():\n",
    "    original_content = row[\"content\"]\n",
    "    new_content, removed_lines, summary = rank_sentences(original_content)\n",
    "    results = results.append({\"Original Content\": original_content, \"New Content\": new_content, \"Removed Lines\": removed_lines, \"Further Metrics\": summary}, ignore_index=True)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results.to_csv(\"test_results.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
